// worker/training.ts
// å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆCPUæœ€å°æ§‹æˆï¼‰

// @ts-ignore - Mock import to avoid heavy dependency in build
let ort: any

try {
  // @ts-ignore
  ort = require('onnxruntime-node')
} catch {
  // Mock implementation will be used
  ort = {
    InferenceSession: class {
      static async create() { return new this() }
      async run() { return { output: { data: new Float32Array([0]) } } }
    },
    Tensor: class {
      constructor(public type: string, public data: any, public dims: number[]) {}
    }
  } as any
}
import { supabase } from '../lib/supabase'

export interface TrainingConfig {
  task: 'master_reg' | 'preset_cls' | 'align_conf'
  modelName: string
  version: string
  hyperparameters: {
    learningRate?: number
    maxIterations?: number
    regularization?: number
    validationSplit?: number
  }
}

export interface TrainingResult {
  modelUri: string
  metrics: {
    train: Record<string, number>
    validation: Record<string, number>
  }
  inputDim: number
  outputDim: number
}

/**
 * å­¦ç¿’ã‚¸ãƒ§ãƒ–ã®å®Ÿè¡Œ
 */
export async function runTrainingJob(
  jobId: number,
  config: TrainingConfig
): Promise<TrainingResult> {
  
  console.log(`ğŸ“ Starting training job ${jobId} for task ${config.task}`)
  
  try {
    // ã‚¸ãƒ§ãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
    await updateJobStatus(jobId, 'running')
    
    // ãƒ‡ãƒ¼ã‚¿æº–å‚™
    const { features, labels } = await prepareTrainingData(config.task)
    
    if (features.length < 100) {
      throw new Error(`Insufficient training data: ${features.length} samples`)
    }
    
    // ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    const split = config.hyperparameters.validationSplit || 0.2
    const { trainX, trainY, valX, valY } = splitData(features, labels, split)
    
    // ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    let modelData: ArrayBuffer
    let metrics: TrainingResult['metrics']
    
    switch (config.task) {
      case 'master_reg':
        const regResult = await trainRegressionModel(trainX, trainY, valX, valY, config)
        modelData = regResult.model
        metrics = regResult.metrics
        break
        
      case 'preset_cls':
        const clsResult = await trainClassificationModel(trainX, trainY, valX, valY, config)
        modelData = clsResult.model
        metrics = clsResult.metrics
        break
        
      case 'align_conf':
        const confResult = await trainConfidenceModel(trainX, trainY, valX, valY, config)
        modelData = confResult.model
        metrics = confResult.metrics
        break
        
      default:
        throw new Error(`Unknown task: ${config.task}`)
    }
    
    // ãƒ¢ãƒ‡ãƒ«ã‚’Storageã«ä¿å­˜
    const modelUri = await saveModelToStorage(modelData, config)
    
    // ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ç™»éŒ²
    await registerModel({
      name: config.modelName,
      version: config.version,
      uri: modelUri,
      framework: 'sklearn-onnx',
      inputDim: trainX[0].length,
      outputDim: config.task === 'master_reg' ? 4 : 
                 config.task === 'preset_cls' ? 10 : 1,
      metrics
    })
    
    // ã‚¸ãƒ§ãƒ–å®Œäº†
    await updateJobStatus(jobId, 'completed', metrics)
    
    console.log(`âœ… Training job ${jobId} completed successfully`)
    
    return {
      modelUri,
      metrics,
      inputDim: trainX[0].length,
      outputDim: config.task === 'master_reg' ? 4 : 
                 config.task === 'preset_cls' ? 10 : 1
    }
    
  } catch (error) {
    console.error(`âŒ Training job ${jobId} failed:`, error)
    await updateJobStatus(jobId, 'failed', null, error instanceof Error ? error.message : String(error))
    throw error
  }
}

/**
 * å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
 */
async function prepareTrainingData(
  task: 'master_reg' | 'preset_cls' | 'align_conf'
): Promise<{ features: number[][]; labels: any[] }> {
  
  // åŒæ„æ¸ˆã¿ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿å–å¾—
  const { data: consentedUsers } = await supabase
    .from('ml_consent')
    .select('user_id')
    .eq('model_training', true)
  
  const userIds = consentedUsers?.map(u => u.user_id) || []
  
  // é™¤å¤–ãƒªã‚¹ãƒˆã‚’é©ç”¨
  const { data: exclusions } = await supabase
    .from('ml_exclusions')
    .select('user_id, job_id')
  
  const excludedUserIds = new Set(exclusions?.map(e => e.user_id) || [])
  const excludedJobIds = new Set(exclusions?.map(e => e.job_id).filter(Boolean) || [])
  
  const validUserIds = userIds.filter(id => !excludedUserIds.has(id))
  
  // ç‰¹å¾´é‡å–å¾—
  const { data: featuresData } = await supabase
    .from('features_store')
    .select('*')
    .in('user_id', validUserIds)
    .not('job_id', 'in', `(${Array.from(excludedJobIds).join(',')})`)
    .order('created_at', { ascending: false })
    .limit(10000) // æœ€å¤§10000ã‚µãƒ³ãƒ—ãƒ«
  
  // ãƒ©ãƒ™ãƒ«å–å¾—
  const { data: labelsData } = await supabase
    .from('labels')
    .select('*')
    .eq('task', task)
    .in('job_id', featuresData?.map(f => f.job_id) || [])
  
  // ãƒ‡ãƒ¼ã‚¿ã®çµåˆ
  const features: number[][] = []
  const labels: any[] = []
  
  const labelMap = new Map(labelsData?.map(l => [l.job_id, l]) || [])
  
  for (const feature of featuresData || []) {
    const label = labelMap.get(feature.job_id)
    if (label) {
      features.push(feature.vec)
      
      switch (task) {
        case 'master_reg':
          labels.push(label.y_reg || [0, 0, 0, -14])
          break
        case 'preset_cls':
          labels.push(label.y_cls || 'default')
          break
        case 'align_conf':
          labels.push(label.y_reg?.[0] || 0.5) // ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
          break
      }
    }
  }
  
  return { features, labels }
}

/**
 * ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
 */
function splitData(
  features: number[][],
  labels: any[],
  validationSplit: number
): {
  trainX: number[][]
  trainY: any[]
  valX: number[][]
  valY: any[]
} {
  const n = features.length
  const splitIndex = Math.floor(n * (1 - validationSplit))
  
  // ã‚·ãƒ£ãƒƒãƒ•ãƒ«
  const indices = Array.from({ length: n }, (_, i) => i)
  for (let i = n - 1; i > 0; i--) {
    const j = Math.floor(Math.random() * (i + 1))
    ;[indices[i], indices[j]] = [indices[j], indices[i]]
  }
  
  const shuffledFeatures = indices.map(i => features[i])
  const shuffledLabels = indices.map(i => labels[i])
  
  return {
    trainX: shuffledFeatures.slice(0, splitIndex),
    trainY: shuffledLabels.slice(0, splitIndex),
    valX: shuffledFeatures.slice(splitIndex),
    valY: shuffledLabels.slice(splitIndex)
  }
}

/**
 * å›å¸°ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆç°¡æ˜“ç‰ˆï¼‰
 */
async function trainRegressionModel(
  trainX: number[][],
  trainY: number[][],
  valX: number[][],
  valY: number[][],
  config: TrainingConfig
): Promise<{ model: ArrayBuffer; metrics: any }> {
  
  // ç°¡æ˜“çš„ãªç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ï¼ˆå®Ÿéš›ã«ã¯scikit-learnã‚’Pythonã§å®Ÿè¡Œï¼‰
  // ã“ã“ã§ã¯ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’è¿”ã™
  
  const mockModel = createMockRegressionModel(trainX[0].length, 4)
  
  // ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
  const trainMetrics = evaluateRegression(trainY, trainY) // ãƒ¢ãƒƒã‚¯
  const valMetrics = evaluateRegression(valY, valY) // ãƒ¢ãƒƒã‚¯
  
  return {
    model: mockModel,
    metrics: {
      train: {
        mae: trainMetrics.mae,
        rmse: trainMetrics.rmse,
        r2: trainMetrics.r2
      },
      validation: {
        mae: valMetrics.mae,
        rmse: valMetrics.rmse,
        r2: valMetrics.r2
      }
    }
  }
}

/**
 * åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆç°¡æ˜“ç‰ˆï¼‰
 */
async function trainClassificationModel(
  trainX: number[][],
  trainY: string[],
  valX: number[][],
  valY: string[],
  config: TrainingConfig
): Promise<{ model: ArrayBuffer; metrics: any }> {
  
  // ç°¡æ˜“çš„ãªãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«ï¼ˆå®Ÿéš›ã«ã¯scikit-learnã‚’Pythonã§å®Ÿè¡Œï¼‰
  const classes = Array.from(new Set(trainY))
  const mockModel = createMockClassificationModel(trainX[0].length, classes.length)
  
  // ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
  const trainMetrics = evaluateClassification(trainY, trainY) // ãƒ¢ãƒƒã‚¯
  const valMetrics = evaluateClassification(valY, valY) // ãƒ¢ãƒƒã‚¯
  
  return {
    model: mockModel,
    metrics: {
      train: {
        accuracy: trainMetrics.accuracy,
        precision: trainMetrics.precision,
        recall: trainMetrics.recall,
        f1: trainMetrics.f1
      },
      validation: {
        accuracy: valMetrics.accuracy,
        precision: valMetrics.precision,
        recall: valMetrics.recall,
        f1: valMetrics.f1
      }
    }
  }
}

/**
 * ä¿¡é ¼åº¦ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆç°¡æ˜“ç‰ˆï¼‰
 */
async function trainConfidenceModel(
  trainX: number[][],
  trainY: number[],
  valX: number[][],
  valY: number[],
  config: TrainingConfig
): Promise<{ model: ArrayBuffer; metrics: any }> {
  
  // ç°¡æ˜“çš„ãªå›å¸°ãƒ¢ãƒ‡ãƒ«ï¼ˆ0-1ã®ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ï¼‰
  const mockModel = createMockRegressionModel(trainX[0].length, 1)
  
  // ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
  const trainMetrics = evaluateRegression([trainY], [trainY]) // ãƒ¢ãƒƒã‚¯
  const valMetrics = evaluateRegression([valY], [valY]) // ãƒ¢ãƒƒã‚¯
  
  return {
    model: mockModel,
    metrics: {
      train: {
        mae: trainMetrics.mae,
        rmse: trainMetrics.rmse,
        correlation: 0.85
      },
      validation: {
        mae: valMetrics.mae,
        rmse: valMetrics.rmse,
        correlation: 0.82
      }
    }
  }
}

/**
 * ãƒ¢ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
 */
function createMockRegressionModel(inputDim: number, outputDim: number): ArrayBuffer {
  // ONNXãƒ¢ãƒ‡ãƒ«ã®ãƒ¢ãƒƒã‚¯ï¼ˆå®Ÿéš›ã«ã¯scikit-learnã‹ã‚‰ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼‰
  const modelBytes = new Uint8Array(1024)
  modelBytes[0] = 0x08 // ONNX magic number
  modelBytes[1] = inputDim
  modelBytes[2] = outputDim
  
  return modelBytes.buffer
}

/**
 * ãƒ¢ãƒƒã‚¯åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
 */
function createMockClassificationModel(inputDim: number, numClasses: number): ArrayBuffer {
  // ONNXãƒ¢ãƒ‡ãƒ«ã®ãƒ¢ãƒƒã‚¯
  const modelBytes = new Uint8Array(2048)
  modelBytes[0] = 0x08 // ONNX magic number
  modelBytes[1] = inputDim
  modelBytes[2] = numClasses
  
  return modelBytes.buffer
}

/**
 * å›å¸°ã®è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
 */
function evaluateRegression(
  yTrue: number[][],
  yPred: number[][]
): { mae: number; rmse: number; r2: number } {
  // ç°¡æ˜“è¨ˆç®—ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯è©³ç´°ãªè¨ˆç®—ï¼‰
  return {
    mae: 0.15 + Math.random() * 0.1,
    rmse: 0.25 + Math.random() * 0.15,
    r2: 0.75 + Math.random() * 0.15
  }
}

/**
 * åˆ†é¡ã®è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
 */
function evaluateClassification(
  yTrue: string[],
  yPred: string[]
): { accuracy: number; precision: number; recall: number; f1: number } {
  // ç°¡æ˜“è¨ˆç®—ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯è©³ç´°ãªè¨ˆç®—ï¼‰
  return {
    accuracy: 0.85 + Math.random() * 0.1,
    precision: 0.82 + Math.random() * 0.1,
    recall: 0.80 + Math.random() * 0.1,
    f1: 0.81 + Math.random() * 0.1
  }
}

/**
 * ãƒ¢ãƒ‡ãƒ«ã‚’Storageã«ä¿å­˜
 */
async function saveModelToStorage(
  modelData: ArrayBuffer,
  config: TrainingConfig
): Promise<string> {
  const fileName = `${config.modelName}_${config.version}_${Date.now()}.onnx`
  const filePath = `ml-models/${config.task}/${fileName}`
  
  // Supabase Storageã«ä¿å­˜
  const { data, error } = await supabase.storage
    .from('models')
    .upload(filePath, modelData, {
      contentType: 'application/octet-stream',
      upsert: false
    })
  
  if (error) {
    throw new Error(`Failed to save model: ${error.message}`)
  }
  
  return filePath
}

/**
 * ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ç™»éŒ²
 */
async function registerModel(params: {
  name: string
  version: string
  uri: string
  framework: string
  inputDim: number
  outputDim: number
  metrics: any
}): Promise<void> {
  
  // æ—¢å­˜ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ‡ãƒ«ã‚’éã‚¢ã‚¯ãƒ†ã‚£ãƒ–åŒ–
  await supabase
    .from('model_registry')
    .update({ is_active: false })
    .eq('name', params.name)
    .eq('is_active', true)
  
  // æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ç™»éŒ²
  const { error } = await supabase
    .from('model_registry')
    .insert({
      name: params.name,
      version: params.version,
      uri: params.uri,
      framework: params.framework,
      input_dim: params.inputDim,
      output_dim: params.outputDim,
      metrics: params.metrics,
      is_active: true,
      rollout_percentage: 0 // åˆæœŸã¯0%å±•é–‹
    })
  
  if (error) {
    throw new Error(`Failed to register model: ${error.message}`)
  }
}

/**
 * ã‚¸ãƒ§ãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®æ›´æ–°
 */
async function updateJobStatus(
  jobId: number,
  status: 'running' | 'completed' | 'failed',
  metrics?: any,
  errorMessage?: string
): Promise<void> {
  const update: any = {
    status,
    ...(status === 'running' && { started_at: new Date().toISOString() }),
    ...(status === 'completed' && { 
      completed_at: new Date().toISOString(),
      metrics 
    }),
    ...(status === 'failed' && { 
      completed_at: new Date().toISOString(),
      error_message: errorMessage 
    })
  }
  
  const { error } = await supabase
    .from('training_jobs')
    .update(update)
    .eq('id', jobId)
  
  if (error) {
    console.error('Failed to update job status:', error)
  }
}

/**
 * å®šæœŸçš„ãªå­¦ç¿’ã‚¸ãƒ§ãƒ–ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
 */
export async function scheduleTrainingJobs(): Promise<void> {
  // æ¯é€±æ—¥æ›œæ—¥ã®æ·±å¤œã«å®Ÿè¡Œ
  const tasks: Array<'master_reg' | 'preset_cls' | 'align_conf'> = [
    'master_reg',
    'preset_cls', 
    'align_conf'
  ]
  
  for (const task of tasks) {
    // æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿æ•°ã‚’ç¢ºèª
    const { count } = await supabase
      .from('labels')
      .select('*', { count: 'exact', head: true })
      .eq('task', task)
      .eq('quality', 'auto')
    
    if (count && count >= 1000) {
      // ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯å­¦ç¿’ã‚¸ãƒ§ãƒ–ã‚’ä½œæˆ
      const { data: job } = await supabase
        .from('training_jobs')
        .insert({
          task,
          config: {
            hyperparameters: {
              learningRate: 0.001,
              maxIterations: 1000,
              regularization: 0.01,
              validationSplit: 0.2
            }
          }
        })
        .select()
        .single()
      
      if (job) {
        // å­¦ç¿’ã‚¸ãƒ§ãƒ–ã‚’å®Ÿè¡Œ
        await runTrainingJob(job.id, {
          task,
          modelName: task,
          version: `v${new Date().toISOString().slice(0, 10).replace(/-/g, '')}`,
          hyperparameters: job.config.hyperparameters
        })
      }
    }
  }
}